---
layout: single
title: "Remote Process Group으로 중첩 데이터 구조 간소화"
date: 2025-09-06
categories:
  - Nifi
tags:
  - Nifi
  - FlowFile
toc: true
toc_sticky: true
---

NiFi에서 데이터 플로우를 설계할 때, 관련된 프로세서들을 묶어 논리적으로 명확한 파이프라인을 구성할 수 있게 해주는 '프로세스 그룹(Process Group)'은 필수적인 기능입니다. 

그러나, 여러 프로세스 그룹에서 처리된 결과를 최종적으로 로깅하거나 통합 처리해야 하는 경우에는 문제가 발생합니다.

각 중첩 레벨마다 Output Port를 만들어 외부로 데이터를 꺼내오는 방식은 그룹의 깊이가 깊어질수록 설정이 번거로워지고 전체 플로우의 직관성을 떨어뜨리기 때문입니다.

이번 포스트에서는 이러한 문제를 해결하기 위한 방법 중 하나인 **Remote Process Group**를 통해 데이터 플로우를 NiFi의 최상단으로 직접 전송하는 법을 알아보겠습니다.


## Remote Process Group이란?

**Remote Process Group(RPG)**은 한마디로 NiFi의 '데이터 포털'입니다. 이걸 사용하면 현재 NiFi 인턴스에서 **다른 NiFi 인턴스로 데이터를 보내거나 받을 수 있습니다.**

또 하나의 기능은 바로 자기 자신, 즉 **`localhost`로 데이터를 다시 보내는 '루프백(Loopback)'** 기법입니다. 복잡하게 얽힌 프로세스 그룹을 깔끔하게 정리하거나, 공통 로직을 하나의 모듈처럼 만들어 재사용하고 싶을 때 유용하게 쓰이는 기술입니다.



## 주요 설정 및 데이터 수신 방법

RPG를 이용해 현재 NiFi의 최상단으로 데이터를 보내는 설정은 아주 간단합니다. 먼저 RPG를 캔버스에 추가하고 아래처럼 설정합니다.

### 1. 데이터 수신 위치에 Input port를 생성

RPG가 보낸 데이터는 **Input Port**를 통해 수신해야 합니다. 데이터를 받고자 하는 위치(주로 최상단 캔버스)에 Input Port를 추가하고, 이름을 지정합니다. 

### 2. Remote Process Group 설정

* **URLs**: 원격으로 접속할 NiFi의 주소를 입력합니다. 자기 자신에게 보낼 것이므로 `http://localhost:8080/nifi`와 같이 현재 NiFi의 주소를 적어줍니다.
*(만약 NiFi 보안 설정을 했다면 `https://localhost:9443/nifi`와 같이 https와 포트 번호를 맞춰주세요.)*
* **Transport Protocol**: NiFi 1.0.0 버전 이후부터는 기본적으로 HTTP 통신을 사용하므로 **`HTTP`**로 설정합니다.
* **활성화**: 설정이 끝나면 RPG 컴포넌트를 우클릭하여 **`Enable transmission`**을 선택해 전송을 활성화합니다.
* **플로우를 port에 할당** : 위에서 생성한 Input Port를 선택하면, 데이터를 해당 포트로 흘려보낼 수 있습니다.

이 설정으로 RPG는 지정된 주소의 Input port로 NiFi로 데이터를 보낼 수 있게 됩니다.

## 사용 예시

가장 대표적인 예시는 도입부에서 언급한 것과 같이, **깊게 중첩된 프로세스 그룹에서 로그나 처리 결과를 최상단으로 한 번에 보내는 경우**입니다.

4~5단계 깊이의 프로세스 그룹에서 처리된 최종 결과물을 로깅하게 된다면, 일반적인 'Output Port'를 사용한다면 각 단계를 일일이 거쳐 나와야 해서 매우 번거롭습니다. 하지만 RPG를 사용하면 가장 깊은 곳에 있는 프로세스 그룹에서 RPG로 데이터를 연결하기만 해도 최상단 캔버스에 있는 Input Port로 데이터가 바로 도착합니다.

`[개인 nifi 이미지 삽입 예정]`

---

## 장점

* **흐름의 단순화 및 가독성 향상**: 여러 단계의 Output Port 연결을 생략할 수 있어, 아무리 복잡한 구조라도 데이터의 최종 목적지를 명확하고 깔끔하게 만들 수 있음.
* **모듈화 및 재사용성 증가**: 공통으로 사용되는 로직(에러 처리, 로깅 등)을 최상단에 Input Port로 만들어두고, 여러 프로세스 '모듈'에서 RPG를 통해 데이터를 보내는 방식으로 재사용성을 높임.
* **클러스터 환경에서의 로드 밸런싱**: NiFi 클러스터 환경에서 RPG를 `localhost`로 연결하면, 데이터가 클러스터 내의 여러 노드에 자동으로 분산되어 부하를 균등하게 만들 수 있음.

---

## 단점

* **네트워크 오버헤드 발생**: 내부에 있는 컴포넌트끼리 연결하는 것과 달리, RPG는 로컬 환경이라도 **내부적으로 네트워크 통신(S2S 프로토콜)을 사용**함. 이로 인해 데이터 직렬화/역직렬화 과정이 추가되어 약간의 성능 저하 및 리소스 사용이 발생.
* **설정의 번거로움**: 단순히 선을 긋는 것보다 URL 입력, Input Port 생성 등 추가적인 설정 단계가 필요.
* **분리된 트랜잭션**: 데이터가 RPG를 통과하는 순간 하나의 트랜잭션이 끝나고, Input Port에서 새로운 트랜잭션으로 시작됨. 이 때문에 데이터의 흐름을 추적할 때 한 단계가 더 생긴 것처럼 보일 수 있음.

---

## 결론
 